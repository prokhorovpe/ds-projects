"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö.
–î–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –∏ –æ–±—É—á–∞–µ—Ç—Å—è –ø–æ–¥—Ö–æ–¥—è—â–∞—è –º–æ–¥–µ–ª—å:
- –î–ª—è HIGH_FREQUENCY_MODALITIES: –ì–∏–±—Ä–∏–¥–Ω–∞—è –º–æ–¥–µ–ª—å Prophet + XGBoost –¥–ª—è –æ—Å—Ç–∞—Ç–∫–æ–≤.
- –î–ª—è SPECIALIZED_MODALITIES: –ú–æ–¥–µ–ª—å –¥–µ—Ç–µ–∫—Ü–∏–∏ –≤—Å–ø–ª–µ—Å–∫–æ–≤ (XGBoost Classifier + 2 XGBoost Regressor).
–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è (–º–æ–¥–µ–ª–∏, –º–µ—Ç—Ä–∏–∫–∏) —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –Ω–∞ –¥–∏—Å–∫.
"""
import pandas as pd
import numpy as np
import pickle
import os
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning) # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ—Ç Prophet –∏ sklearn.
from prophet import Prophet
from xgboost import XGBRegressor, XGBClassifier
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
from config import (
    MODEL_DIR, RESULTS_DIR, TEST_START_DATE, HIGH_FREQUENCY_MODALITIES, SPECIALIZED_MODALITIES,
    PAY_WINDOW_START, PAY_WINDOW_END, PAY_WINDOW_MID_START, PAY_WINDOW_MID_END, RU_HOLIDAYS, RANDOM_STATE
)
from data_preprocessor import load_processed_data

def symmetric_mape(y_true, y_pred):
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—É—é MAPE (sMAPE), –∫–æ—Ç–æ—Ä–∞—è –±–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤–∞ –∫ –Ω—É–ª–µ–≤—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º."""
    return np.mean(2 * np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred) + 1e-8)) * 100

# –ü–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
def add_calendar_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –∫ DataFrame —Å –¥–∞—Ç–∞–º–∏ (—Å—Ç–æ–ª–±–µ—Ü 'ds') –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
    –≠—Ç–æ –∫–ª—é—á–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –∫–∞–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏, —Ç–∞–∫ –∏ –ø—Ä–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–∏.
    –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–µ–∂–¥—É —ç—Ç–∞–ø–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ö–†–ò–¢–ò–ß–ï–°–ö–ò –≤–∞–∂–Ω–∞.
    """
    df = df.copy()
    df['ds'] = pd.to_datetime(df['ds']).dt.tz_localize(None)
    
    # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–∞—Ç—ã.
    df['month'] = df['ds'].dt.month.astype('int8')
    df['day_of_month'] = df['ds'].dt.day.astype('int8')
    df['week_of_year'] = df['ds'].dt.isocalendar().week.astype('int8')
    df['quarter'] = df['ds'].dt.quarter.astype('int8')
    df['year'] = df['ds'].dt.year.astype('int16')
    df['dow'] = df['ds'].dt.weekday.astype('int8')  # –î–µ–Ω—å –Ω–µ–¥–µ–ª–∏ (0=–ü–Ω, 6=–í—Å)
    
    # –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫ –±–ª–∏–∂–µ –∫ –≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å—é, —á–µ–º –∫ —Å—Ä–µ–¥–µ).
    df['sin_dow'] = np.sin(2 * np.pi * df['dow'] / 7).astype('float32')
    df['cos_dow'] = np.cos(2 * np.pi * df['dow'] / 7).astype('float32')
    df['sin_month'] = np.sin(2 * np.pi * df['month'] / 12).astype('float32')
    df['sin_day_of_year'] = np.sin(2 * np.pi * df['ds'].dt.dayofyear / 365).astype('float32')
    
    # –ü—Ä–∞–∑–¥–Ω–∏–∫–∏ –∏ –≤—ã—Ö–æ–¥–Ω—ã–µ.
    df['is_holiday'] = df['ds'].isin(RU_HOLIDAYS).astype('int8')
    df['is_weekend'] = (df['dow'] >= 5).astype('int8')  # –°–±=5, –í—Å=6
    
    # –ë–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫–∞: –ø—Ä–∏–∑–Ω–∞–∫–∏ "–æ–∫–æ–Ω –≤—ã–ø–ª–∞—Ç" (–Ω–∞—á–∞–ª–æ –∏ —Å–µ—Ä–µ–¥–∏–Ω–∞ –º–µ—Å—è—Ü–∞).
    df['is_pay_window'] = (
        ((df['day_of_month'] >= PAY_WINDOW_START) & (df['day_of_month'] <= PAY_WINDOW_END)) |
        ((df['day_of_month'] >= PAY_WINDOW_MID_START) & (df['day_of_month'] <= PAY_WINDOW_MID_END))
    ).astype('int8')
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø—Ä–∞–∑–¥–Ω–∏—á–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: —Å–∫–æ–ª—å–∫–æ –¥–Ω–µ–π –¥–æ/–ø–æ—Å–ª–µ –±–ª–∏–∂–∞–π—à–µ–≥–æ –ø—Ä–∞–∑–¥–Ω–∏–∫–∞.
    df['days_to_holiday'] = df['ds'].apply(
        lambda x: min([(h - x).days for h in RU_HOLIDAYS if h > x] or [365])
    ).clip(0, 7).astype('int8')  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 7 –¥–Ω–µ–π.
    df['days_after_holiday'] = df['ds'].apply(
        lambda x: min([(x - h).days for h in RU_HOLIDAYS if h < x] or [365])
    ).clip(0, 7).astype('int8')
    df['near_holiday'] = ((df['days_to_holiday'] <= 2) | (df['days_after_holiday'] <= 2)).astype('int8')
    
    # –ü—Ä–∏–∑–Ω–∞–∫ "–¥–ª–∏–Ω–Ω—ã—Ö" –ø—Ä–∞–∑–¥–Ω–∏–∫–æ–≤ (–µ—Å–ª–∏ –≤ —Ä–∞–¥–∏—É—Å–µ ¬±2 –¥–Ω—è –æ—Ç —Ç–µ–∫—É—â–µ–π –¥–∞—Ç—ã –µ—Å—Ç—å 3+ –ø—Ä–∞–∑–¥–Ω–∏–∫–∞).
    def is_extended_holiday(date):
        count = sum(1 for i in range(-2, 3) if (date + pd.Timedelta(days=i)) in RU_HOLIDAYS)
        return 1 if count >= 3 else 0
    df['is_extended_holiday'] = df['ds'].apply(is_extended_holiday).astype('int8')
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–∏–∑–Ω–µ—Å-–ø—Ä–∏–∑–Ω–∞–∫–∏.
    df['is_month_start'] = (df['day_of_month'] == 1).astype('int8')
    df['is_month_end'] = (df['ds'].dt.is_month_end).astype('int8')
    df['is_quarter_start'] = ((df['month'] - 1) % 3 == 0) & (df['day_of_month'] <= 7).astype('int8')
    df['is_quarter_end'] = ((df['month'] % 3 == 0) & (df['day_of_month'] >= 24)).astype('int8')
    
    # –†–∞–±–æ—á–∏–π –¥–µ–Ω—å = –Ω–µ –≤—ã—Ö–æ–¥–Ω–æ–π –∏ –Ω–µ –ø—Ä–∞–∑–¥–Ω–∏–∫.
    df['is_workday'] = (~(df['is_weekend'] | df['is_holiday'])).astype('int8')
    
    return df

# --- –§—É–Ω–∫—Ü–∏–∏ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π ---

def create_prophet_model(modality: str):
    """
    –°–æ–∑–¥–∞–µ—Ç –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å Prophet –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–π –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏.
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥–±–∏—Ä–∞—é—Ç—Å—è –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ —Ç–∏–ø–∞ –º–æ–¥–µ–ª–∏.
    """
    if modality in SPECIALIZED_MODALITIES:
        # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –Ω–∏–∑–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π.
        base_params = {
            'yearly_seasonality': False,
            'weekly_seasonality': True,
            'daily_seasonality': False,
            'seasonality_mode': 'additive',
            'mcmc_samples': 0,
            'changepoint_prior_scale': 0.001, # –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º —Ç—Ä–µ–Ω–¥–∞.
            'growth': 'linear'
        }
    else:
        # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –≤—ã—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π.
        base_params = {
            'yearly_seasonality': False,
            'weekly_seasonality': True,
            'daily_seasonality': False,
            'seasonality_mode': 'multiplicative', # –ú—É–ª—å—Ç–∏–ø–ª–∏–∫–∞—Ç–∏–≤–Ω–∞—è —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å –ª—É—á—à–µ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞—Å—Ç—É—â–µ–π –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å—é.
            'mcmc_samples': 0,
            'growth': 'linear'
        }

    # –ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏.
    modality_params = {
    '–†–ì': {'changepoint_prior_scale': 0.03, 'seasonality_prior_scale': 10.0},
    '–ú–ú–ì': {'changepoint_prior_scale': 0.03, 'seasonality_prior_scale': 8.0},
    '–§–õ–ì': {'seasonality_prior_scale': 12.0},
    '–ö–¢': {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 6.0},
    '–ú–†–¢': {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 5.0},
    '–î–µ–Ω—Å': {'seasonality_prior_scale': 4.0}
    }

    base_params.update(modality_params.get(modality, {}))
    return Prophet(**base_params)

def create_xgboost_model_for_residuals(modality: str):
    """
    –°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å XGBoost –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Å—Ç–∞—Ç–∫–æ–≤ –ø–æ—Å–ª–µ Prophet.
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ (reg_alpha, reg_lambda) –ø–æ–¥–æ–±—Ä–∞–Ω—ã –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è.
    """
    if modality in SPECIALIZED_MODALITIES:
        params = {
            'n_estimators': 200,
            'max_depth': 4,
            'learning_rate': 0.05,
            'reg_alpha': 2.0,  # L1 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
            'reg_lambda': 10.0, # L2 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
            'random_state': RANDOM_STATE,
            'n_jobs': -1
        }
    else:
        params = {
            'n_estimators': 200,
            'max_depth': 3,
            'learning_rate': 0.1,
            'reg_alpha': 1.0,
            'reg_lambda': 5.0,
            'random_state': RANDOM_STATE,
            'n_jobs': -1
        }
    return XGBRegressor(**params)

# –§–£–ù–ö–¶–ò–Ø –î–ï–¢–ï–ö–¶–ò–ò –í–°–ü–õ–ï–°–ö–û–í
def create_classification_regression_model(modality, train_df, test_df):
    """
    –°–æ–∑–¥–∞–µ—Ç –∏ –æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –¥–µ—Ç–µ–∫—Ü–∏–∏ –≤—Å–ø–ª–µ—Å–∫–æ–≤ –¥–ª—è –Ω–∏–∑–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π.
    –ê–ª–≥–æ—Ä–∏—Ç–º:
      1. –ù–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤—ã—è–≤–ª—è—é—Ç—Å—è "–≤—Å–ø–ª–µ—Å–∫–∏" (–∞–Ω–æ–º–∞–ª—å–Ω–æ –≤—ã—Å–æ–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è) —Å –ø–æ–º–æ—â—å—é Z-–æ—Ü–µ–Ω–∫–∏.
      2. –û–±—É—á–∞–µ—Ç—Å—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä (XGBoost) –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è, –±—É–¥–µ—Ç –ª–∏ –≤—Å–ø–ª–µ—Å–∫ –≤ –∑–∞–¥–∞–Ω–Ω—ã–π –¥–µ–Ω—å.
      3. –û–±—É—á–∞—é—Ç—Å—è –¥–≤–µ –º–æ–¥–µ–ª–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏: –¥–ª—è "–Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö" –¥–Ω–µ–π –∏ –¥–ª—è –¥–Ω–µ–π —Å–æ "–≤—Å–ø–ª–µ—Å–∫–∞–º–∏".
      4. –ù–∞ —Ç–µ—Å—Ç–µ: —Å–Ω–∞—á–∞–ª–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –¥–Ω—è, –∑–∞—Ç–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä –¥–µ–ª–∞–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑.
    """
    train_df = train_df.copy()
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏: –º–µ—Å—è—Ü + –¥–µ–Ω—å –Ω–µ–¥–µ–ª–∏.
    train_df['month'] = train_df['ds'].dt.month
    train_df['dow'] = train_df['ds'].dt.weekday
    train_df['group_key'] = train_df['month'].astype(str) + '_' + train_df['dow'].astype(str)
    train_df = train_df.sort_values('ds').reset_index(drop=True)
    train_df['is_spike'] = 0

    # –í—ã—è–≤–ª–µ–Ω–∏–µ –≤—Å–ø–ª–µ—Å–∫–æ–≤: —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã.
    for group_key in train_df['group_key'].unique():
        mask = train_df['group_key'] == group_key
        if mask.sum() < 5:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≥—Ä—É–ø–ø—ã —Å –º–∞–ª—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö.
            continue
        group_data = train_df[mask].copy()
        rolling_mean = group_data['y'].rolling(window=30, min_periods=5).mean()
        rolling_std = group_data['y'].rolling(window=30, min_periods=5).std()
        # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä—è–¥–∞ —Ä–∞—Å—à–∏—Ä—è—é—â–∏–º—Å—è —Å—Ä–µ–¥–Ω–∏–º.
        rolling_mean = rolling_mean.fillna(group_data['y'].expanding().mean())
        rolling_std = rolling_std.fillna(group_data['y'].expanding().std())
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º Z-–æ—Ü–µ–Ω–∫—É.
        z_score = (group_data['y'] - rolling_mean) / (rolling_std + 1e-8)
        # –°–¥–≤–∏–≥–∞–µ–º –Ω–∞ 1, —á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç–µ–∫—É—â–µ–≥–æ.
        z_score_shifted = z_score.shift(1).fillna(False).infer_objects(copy=False)
        # –ú–∞—Å–∫–∞ –≤—Å–ø–ª–µ—Å–∫–æ–≤: Z-–æ—Ü–µ–Ω–∫–∞ > 1.5.
        spike_mask = (z_score_shifted > 1.5)
        train_df.loc[mask, 'is_spike'] = spike_mask.astype(int)

    # Fallback —Å –ø–æ–Ω–∏–∂–µ–Ω–∏–µ–º –ø–æ—Ä–æ–≥–∞
    # –ï—Å–ª–∏ –≤—Å–ø–ª–µ—Å–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –ø—Ä–æ–±—É–µ–º –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–π –º–µ—Ç–æ–¥: –∑–Ω–∞—á–µ–Ω–∏–µ > 1.5 * —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ.
    if train_df['is_spike'].sum() == 0:
        print(f"‚ö†Ô∏è –î–ª—è {modality}: –í—Å–ø–ª–µ—Å–∫–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ. –ü—Ä–æ–±—É–µ–º –ø–æ–Ω–∏–∑–∏—Ç—å –ø–æ—Ä–æ–≥...")
        for group_key in train_df['group_key'].unique():
            mask = train_df['group_key'] == group_key
            if mask.sum() < 5:
                continue
            group_data = train_df[mask].copy()
            rolling_mean = group_data['y'].rolling(window=30, min_periods=5).mean()
            rolling_mean = rolling_mean.fillna(group_data['y'].expanding().mean())
            spike_mask = (group_data['y'] > rolling_mean * 1.5)
            spike_mask = spike_mask.shift(1).fillna(False).infer_objects(copy=False)
            train_df.loc[mask, 'is_spike'] = spike_mask.astype(int)
        # –ï—Å–ª–∏ –∏ –ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ –≤—Å–ø–ª–µ—Å–∫–æ–≤ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é.
        if train_df['is_spike'].sum() == 0:
            print(f"‚ö†Ô∏è –î–ª—è {modality}: –î–∞–Ω–Ω—ã–µ –æ—á–µ–Ω—å –≥–ª–∞–¥–∫–∏–µ. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è.")
            train_df_with_feats = add_calendar_features(train_df)
            test_df_with_feats = add_calendar_features(test_df)
            feature_cols = [col for col in train_df_with_feats.columns if col not in ['ds', 'y', 'is_spike', 'month', 'dow', 'group_key']]
            model = XGBRegressor(n_estimators=150, max_depth=4, random_state=RANDOM_STATE)
            X_train = train_df_with_feats[feature_cols]
            y_train = train_df_with_feats['y']
            model.fit(X_train, y_train)
            X_test = test_df_with_feats[feature_cols]
            final_predictions = model.predict(X_test)
            final_predictions = np.clip(final_predictions, 0, None)
            # –ü–û–°–¢–û–ë–†–ê–ë–û–¢–ö–ê –í FALLBACK
            final_predictions = final_postprocessing(final_predictions, np.zeros_like(final_predictions), test_df['ds'].values, modality, train_df)
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ 4 –∑–Ω–∞—á–µ–Ω–∏—è, —Ç–∞–∫ –∫–∞–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏ –≤—Ç–æ—Ä–æ–π —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è.
            return final_predictions, model, None, None

    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª–∏.
    models = {
        'classifier': XGBClassifier(n_estimators=100, max_depth=3, random_state=RANDOM_STATE),
        'regressor_normal': XGBRegressor(n_estimators=150, max_depth=4, random_state=RANDOM_STATE),
        'regressor_spike': XGBRegressor(n_estimators=150, max_depth=4, random_state=RANDOM_STATE)
    }

    # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏.
    train_df_with_feats = add_calendar_features(train_df)
    feature_cols = [col for col in train_df_with_feats.columns if col not in ['ds', 'y', 'is_spike', 'month', 'dow', 'group_key']]

    # –û–±—É—á–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä.
    X_train_clf = train_df_with_feats[feature_cols]
    y_train_clf = train_df_with_feats['is_spike']
    models['classifier'].fit(X_train_clf, y_train_clf)

    # –û–±—É—á–∞–µ–º —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä –¥–ª—è "–Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö" –¥–Ω–µ–π.
    normal_mask = train_df_with_feats['is_spike'] == 0
    X_train_normal = train_df_with_feats[normal_mask][feature_cols]
    y_train_normal = train_df_with_feats.loc[normal_mask, 'y']
    models['regressor_normal'].fit(X_train_normal, y_train_normal)

    # –û–±—É—á–∞–µ–º —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä –¥–ª—è –¥–Ω–µ–π —Å–æ "–≤—Å–ø–ª–µ—Å–∫–∞–º–∏". –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä –¥–ª—è –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö –¥–Ω–µ–π.
    spike_mask = train_df_with_feats['is_spike'] == 1
    if spike_mask.sum() > 10:
        X_train_spike = train_df_with_feats[spike_mask][feature_cols]
        y_train_spike = train_df_with_feats.loc[spike_mask, 'y']
        models['regressor_spike'].fit(X_train_spike, y_train_spike)
    else:
        models['regressor_spike'] = models['regressor_normal']

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
    test_df_with_feats = add_calendar_features(test_df)
    X_test = test_df_with_feats[feature_cols]
    spike_predictions = models['classifier'].predict(X_test)

    final_predictions = []
    for i, is_spike in enumerate(spike_predictions):
        if is_spike:
            pred = models['regressor_spike'].predict(X_test.iloc[[i]])[0]
        else:
            pred = models['regressor_normal'].predict(X_test.iloc[[i]])[0]
        final_predictions.append(max(0, pred))

    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ—à–∏–±–∫–∏ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤.
    train_predictions = []
    for i in range(len(X_train_clf)):
        is_spike = y_train_clf.iloc[i]
        if is_spike:
            pred = models['regressor_spike'].predict(X_train_clf.iloc[[i]])[0]
        else:
            pred = models['regressor_normal'].predict(X_train_clf.iloc[[i]])[0]
        train_predictions.append(max(0, pred))

    train_predictions = np.array(train_predictions)
    train_actual = train_df_with_feats.loc[X_train_clf.index, 'y'].values
    train_errors = train_actual - train_predictions

    # –†–∞–∑–¥–µ–ª—è–µ–º –æ—à–∏–±–∫–∏ –Ω–∞ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ –∏ –≤—Å–ø–ª–µ—Å–∫–æ–≤—ã–µ.
    normal_errors = train_errors[y_train_clf == 0]
    spike_errors = train_errors[y_train_clf == 1] if (y_train_clf == 1).sum() > 10 else normal_errors

    # –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –†–ê–°–ß–ï–¢–ê –ö–í–ê–ù–¢–ò–õ–ï–ô –° –£–ß–ï–¢–û–ú –ú–ê–õ–û–ì–û –û–ë–™–ï–ú–ê
    def safe_quantiles(errors, q_low=0.05, q_high=0.95):
        """
        –ë–µ–∑–æ–ø–∞—Å–Ω–æ –≤—ã—á–∏—Å–ª—è–µ—Ç –∫–≤–∞–Ω—Ç–∏–ª–∏, –¥–∞–∂–µ –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ.
        –ï—Å–ª–∏ < 5 —Ç–æ—á–µ–∫ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–¥–∏–∞–Ω—É ¬± 2 * MAD (Median Absolute Deviation).
        –≠—Ç–æ –±–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤–æ, —á–µ–º –∫–≤–∞–Ω—Ç–∏–ª–∏, –Ω–∞ –º–∞–ª—ã—Ö –≤—ã–±–æ—Ä–∫–∞—Ö.
        """
        if len(errors) < 5:
            median = np.median(errors)
            mad = np.median(np.abs(errors - median))
            return median - 2 * mad, median + 2 * mad
        else:
            return np.quantile(errors, q_low), np.quantile(errors, q_high)

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∫–≤–∞–Ω—Ç–∏–ª–∏.
    normal_lower_quantile, normal_upper_quantile = safe_quantiles(normal_errors, 0.05, 0.95)
    spike_lower_quantile, spike_upper_quantile = safe_quantiles(spike_errors, 0.05, 0.95)

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ª–æ–≤–∞—Ä—å –∫–≤–∞–Ω—Ç–∏–ª–µ–π. –í–∞–∂–Ω–æ: –¥–ª—è —Ä–∞–±–æ—á–∏—Ö –¥–Ω–µ–π - –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ –æ—à–∏–±–∫–∏, –¥–ª—è –≤—ã—Ö–æ–¥–Ω—ã—Ö - –≤—Å–ø–ª–µ—Å–∫–æ–≤—ã–µ.
    quantiles = {
        'workday': {'lower': normal_lower_quantile, 'upper': normal_upper_quantile},
        'weekend_or_holiday': {'lower': spike_lower_quantile, 'upper': spike_upper_quantile}
    }

    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –º–∞–∫—Å–∏–º—É–º—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ –¥–Ω—è (99.9-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å).
    mask_workday = train_df_with_feats['is_workday'] == 1
    mask_weekend_or_holiday = (train_df_with_feats['is_weekend'] == 1) | (train_df_with_feats['is_holiday'] == 1)
    max_possible_workday = train_df_with_feats.loc[mask_workday, 'y'].quantile(0.999)
    max_possible_weekend_or_holiday = train_df_with_feats.loc[mask_weekend_or_holiday, 'y'].quantile(0.999)
    max_vals = {
        'workday': max_possible_workday,
        'weekend_or_holiday': max_possible_weekend_or_holiday
    }

    # –í–û–ó–í–†–ê–©–ê–ï–ú –¢–û–õ–¨–ö–û 6 –ó–ù–ê–ß–ï–ù–ò–ô
    return (
        np.array(final_predictions),
        models['classifier'],
        models['regressor_normal'],
        models['regressor_spike'],
        quantiles,
        max_vals
    )

#  –§–£–ù–ö–¶–ò–Ø –ê–î–ê–ü–¢–ò–í–ù–û–ô –ü–û–°–¢–û–ë–†–ê–ë–û–¢–ö–ò
def final_postprocessing(predictions, prophet_predictions, test_dates, modality, train_df):
    """
    –§–∏–Ω–∞–ª—å–Ω–∞—è –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–∞. –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑ —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –≤–æ–∑–º–æ–∂–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏.
    1. –î–ª—è –≤—ã—Ö–æ–¥–Ω—ã—Ö –∏ –ø—Ä–∞–∑–¥–Ω–∏—á–Ω—ã—Ö –¥–Ω–µ–π: –æ–±—Ä–µ–∑–∞–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑ –¥–æ 95-–≥–æ –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—è –æ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π.
    2. –î–ª—è –¥–Ω–µ–π, –≥–¥–µ Prophet –ø—Ä–µ–¥—Å–∫–∞–∑–∞–ª –ø–æ—á—Ç–∏ –Ω–æ–ª—å: –æ–±—Ä–µ–∑–∞–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑ –¥–æ –º–µ–¥–∏–∞–Ω—ã –Ω–∏–∑–∫–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏.
    –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –∑–∞–≤—ã—à–µ–Ω–Ω—ã–µ –∏–ª–∏ –Ω–µ–æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ –Ω–∏–∑–∫–∏–µ –ø—Ä–æ–≥–Ω–æ–∑—ã.
    """
    test_df = pd.DataFrame({'ds': test_dates})
    test_features = add_calendar_features(test_df)
    # –ú–∞—Å–∫–∞ –¥–ª—è –≤—ã—Ö–æ–¥–Ω—ã—Ö –∏ –ø—Ä–∞–∑–¥–Ω–∏–∫–æ–≤.
    mask = (test_features['is_weekend'] == 1) | (test_features['is_holiday'] == 1)
    if mask.any():
        train_mask = (train_df['is_weekend'] == 1) | (train_df['is_holiday'] == 1)
        if train_mask.sum() > 10:
            max_val = train_df.loc[train_mask, 'y'].quantile(0.95)
        else:
            max_val = train_df['y'].median()
        predictions[mask] = np.minimum(predictions[mask], max_val)

    # –ú–∞—Å–∫–∞ –¥–ª—è –¥–Ω–µ–π —Å –æ—á–µ–Ω—å –Ω–∏–∑–∫–∏–º –ø—Ä–æ–≥–Ω–æ–∑–æ–º –æ—Ç Prophet.
    low_threshold = train_df['y'].quantile(0.05) * 0.1
    zero_prophet_mask = prophet_predictions < low_threshold
    if zero_prophet_mask.any():
        low_load_mask = train_df['y'] < train_df['y'].quantile(0.1)
        if low_load_mask.sum() > 5:
            safe_median = train_df.loc[low_load_mask, 'y'].median()
        else:
            safe_median = train_df['y'].median() * 0.1
        predictions[zero_prophet_mask] = np.minimum(predictions[zero_prophet_mask], safe_median)

    return predictions

# --- –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è ---

def run_hybrid_model_for_high_freq(train_df, test_df, modality):
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—É—é –º–æ–¥–µ–ª—å (Prophet + XGBoost) –¥–ª—è –≤—ã—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π.
    """
    train_df = add_calendar_features(train_df)
    test_df = add_calendar_features(test_df)

    model = create_prophet_model(modality)
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–∏–µ —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä—ã –µ—Å—Ç—å –≤ –¥–∞–Ω–Ω—ã—Ö –∏ –¥–æ–±–∞–≤–ª—è–µ–º –∏—Ö –≤ –º–æ–¥–µ–ª—å Prophet.
    prophet_regressors = [
        'is_holiday', 'is_weekend', 'is_extended_holiday', 
        'near_holiday', 'sin_dow', 'is_pay_window', 'sin_month'
    ]
    for regressor in prophet_regressors:
        if regressor in train_df.columns:
            model.add_regressor(regressor)

    # –û–±—É—á–∞–µ–º Prophet –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
    model.fit(train_df[['ds', 'y'] + prophet_regressors])
    future = test_df[['ds'] + prophet_regressors].copy()
    forecast = model.predict(future)

    # –û–ë–†–ï–ó–ö–ê –í–°–ï–• –ö–û–ú–ü–û–ù–ï–ù–¢–û–í –ü–†–û–ì–ù–û–ó–ê
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π –º–∞–∫—Å–∏–º—É–º –¥–ª—è –æ–±—Ä–µ–∑–∫–∏.
    max_possible_value = train_df['y'].quantile(0.999)
    # –û–±—Ä–µ–∑–∞–µ–º –í–°–ï –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ø—Ä–æ–≥–Ω–æ–∑–∞: yhat, yhat_lower, yhat_upper.
    forecast['yhat'] = np.clip(forecast['yhat'], 0, max_possible_value)
    forecast['yhat_lower'] = np.clip(forecast['yhat_lower'], 0, max_possible_value)
    forecast['yhat_upper'] = np.clip(forecast['yhat_upper'], 0, max_possible_value)

    prophet_predictions = forecast.set_index('ds')['yhat']

    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ—Å—Ç–∞—Ç–∫–∏ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ.
    train_forecast = model.predict(train_df[['ds'] + prophet_regressors])
    train_forecast['yhat'] = np.clip(train_forecast['yhat'], 0, None)
    train_residuals = train_df.set_index('ds')['y'] - train_forecast.set_index('ds')['yhat']

    # –û–±—É—á–∞–µ–º XGBoost –Ω–∞ –æ—Å—Ç–∞—Ç–∫–∞—Ö.
    xgb_model = create_xgboost_model_for_residuals(modality)
    xgb_features = [col for col in train_df.columns if col not in ['ds', 'y']]
    X_train = train_df.set_index('ds')[xgb_features].fillna(0)
    y_train = train_residuals.fillna(0)
    sample_weights = np.abs(y_train) + 1  # –í–µ—Å–∞ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω—ã –∞–±—Å–æ–ª—é—Ç–Ω–æ–º—É –∑–Ω–∞—á–µ–Ω–∏—é –æ—Å—Ç–∞—Ç–∫–∞.
    xgb_model.fit(X_train, y_train, sample_weight=sample_weights)

    # –ü—Ä–æ–≥–Ω–æ–∑ –æ—Å—Ç–∞—Ç–∫–æ–≤ –Ω–∞ —Ç–µ—Å—Ç–µ.
    X_test = test_df.set_index('ds')[xgb_features].fillna(0)
    resid_forecast = xgb_model.predict(X_test)

    # –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ = –ø—Ä–æ–≥–Ω–æ–∑ Prophet + –ø—Ä–æ–≥–Ω–æ–∑ –æ—Å—Ç–∞—Ç–∫–æ–≤.
    final_forecast = prophet_predictions.values + resid_forecast

    # –ê–î–ê–ü–¢–ò–í–ù–ê–Ø –ü–û–°–¢–û–ë–†–ê–ë–û–¢–ö–ê
    final_forecast = final_postprocessing(final_forecast, prophet_predictions.values, test_df['ds'].values, modality, train_df)
    final_forecast = np.clip(final_forecast, 0, None)

    # –ì–ï–ù–ï–†–ê–¶–ò–Ø –ê–î–ê–ü–¢–ò–í–ù–´–• –ò–ù–¢–ï–†–í–ê–õ–û–í –ù–ê –¢–ï–°–¢–ï
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ—à–∏–±–∫–∏ –≥–∏–±—Ä–∏–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ.
    train_final_forecast = train_forecast.set_index('ds')['yhat'].values + xgb_model.predict(X_train)
    train_errors = train_df['y'].values - train_final_forecast
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–≤–∞–Ω—Ç–∏–ª–∏ –æ—à–∏–±–æ–∫.
    lower_quantile = np.quantile(train_errors, 0.05)
    upper_quantile = np.quantile(train_errors, 0.95)
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–≤–∞–Ω—Ç–∏–ª–∏ –∫ —Ç–æ—á–µ—á–Ω–æ–º—É –ø—Ä–æ–≥–Ω–æ–∑—É –Ω–∞ —Ç–µ—Å—Ç–µ.
    test_yhat = prophet_predictions.values
    test_resid = resid_forecast
    test_final_forecast = test_yhat + test_resid
    yhat_lower = test_final_forecast + lower_quantile
    yhat_upper = test_final_forecast + upper_quantile
    # –û–±—Ä–µ–∑–∞–µ–º –¥–æ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π.
    max_possible_value = train_df['y'].quantile(0.999)
    yhat_lower = np.clip(yhat_lower, 0, max_possible_value)
    yhat_upper = np.clip(yhat_upper, 0, max_possible_value)
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑.
    interval_forecast = pd.DataFrame({
        'ds': test_df['ds'].values,
        'yhat_lower': yhat_lower,
        'yhat_upper': yhat_upper
    })


    return final_forecast, interval_forecast, model, xgb_model

def run_spike_detection_model_for_low_freq(train_df, test_df, modality):
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –≤—Å–ø–ª–µ—Å–∫–æ–≤ + —Ä–µ–≥—Ä–µ—Å—Å–∏—è) –¥–ª—è –Ω–∏–∑–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π.
    """
    # –ü–æ–ª—É—á–∞–µ–º –∫–≤–∞–Ω—Ç–∏–ª–∏ –∏ –º–∞–∫—Å–∏–º—É–º—ã, —Ä–∞–∑–±–∏—Ç—ã–µ –ø–æ —Ç–∏–ø—É –¥–Ω—è
    final_predictions, classifier_model, regressor_normal, regressor_spike, quantiles, max_vals = create_classification_regression_model(modality, train_df, test_df)

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
    test_df_with_feats = add_calendar_features(test_df)
    feature_cols = [col for col in test_df_with_feats.columns if col not in ['ds', 'y', 'is_spike', 'month', 'dow', 'group_key']]
    X_test = test_df_with_feats[feature_cols]
    spike_predictions = classifier_model.predict(X_test)

    # –ê–î–ê–ü–¢–ò–í–ù–ê–Ø –ì–ï–ù–ï–†–ê–¶–ò–Ø –ò–ù–¢–ï–†–í–ê–õ–û–í –ü–û –¢–ò–ü–£ –î–ù–Ø
    yhat_lower_list = []
    yhat_upper_list = []
    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ —Ç–∏–ø–∞ –¥–Ω—è –∫ —Ç–µ—Å—Ç–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º (–¥—É–±–ª–∏—Ä—É–µ–º –ª–æ–≥–∏–∫—É –∏–∑ add_calendar_features –¥–ª—è —è—Å–Ω–æ—Å—Ç–∏).
    test_df_with_feats['is_weekend'] = (test_df_with_feats['dow'] >= 5).astype('int8')
    test_df_with_feats['is_holiday'] = test_df_with_feats['ds'].isin(RU_HOLIDAYS).astype('int8')
    test_df_with_feats['is_workday'] = ~(test_df_with_feats['is_weekend'] | test_df_with_feats['is_holiday'])

    for i, is_spike in enumerate(spike_predictions):
        pred = final_predictions[i]
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–Ω—è –¥–ª—è —ç—Ç–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞.
        is_workday = test_df_with_feats.iloc[i]['is_workday']
        if is_workday:
            q_lower = quantiles['workday']['lower']
            q_upper = quantiles['workday']['upper']
            max_val = max_vals['workday']
        else:
            q_lower = quantiles['weekend_or_holiday']['lower']
            q_upper = quantiles['weekend_or_holiday']['upper']
            max_val = max_vals['weekend_or_holiday']
        # –í—ã—á–∏—Å–ª—è–µ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª.
        lower_bound = max(0, pred + q_lower)
        upper_bound = pred + q_upper
        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –º–∞–∫—Å–∏–º—É–º–∞.
        lower_bound = min(lower_bound, max_val)
        upper_bound = min(upper_bound, max_val)
        yhat_lower_list.append(lower_bound)
        yhat_upper_list.append(upper_bound)

    interval_forecast = pd.DataFrame({
        'ds': test_df['ds'].values,
        'yhat_lower': yhat_lower_list,
        'yhat_upper': yhat_upper_list
    })

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º 6 –∑–Ω–∞—á–µ–Ω–∏–π, –≤–∫–ª—é—á–∞—è quantiles –∏ max_vals –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.
    return final_predictions, interval_forecast, classifier_model, regressor_normal, quantiles, max_vals

def save_model(model, filename):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å (–∏–ª–∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç) –≤ —Ñ–∞–π–ª –≤ —Ñ–æ—Ä–º–∞—Ç–µ pickle."""
    filepath = os.path.join(MODEL_DIR, filename)
    with open(filepath, 'wb') as f:
        pickle.dump(model, f)
    print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filepath}")

def train_and_evaluate_models():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è. –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ, —Ä–∞–∑–±–∏–≤–∞–µ—Ç –Ω–∞ train/test, –æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª–∏ –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π,
    —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ Excel-—Ñ–∞–π–ª.
    """
    service_time_series = load_processed_data()
    test_start = pd.to_datetime(TEST_START_DATE)
    results = {}
    forecast_tables = {}

    for modality in service_time_series.keys():
        print(f"\n{'='*60}")
        print(f"üéØ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è {modality}")
        print(f"{'='*60}")

        ts_data = service_time_series[modality]

        # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö: –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ —Ñ–æ—Ä–º–∞—Ç—É DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ ['ds', 'y'].
        if isinstance(ts_data, pd.Series):
            ts_data = ts_data.reset_index()
            ts_data.columns = ['ds', 'y']
        elif isinstance(ts_data, pd.DataFrame):
            if len(ts_data.columns) == 1:
                ts_data = ts_data.reset_index()
                ts_data.columns = ['ds', 'y']
            elif len(ts_data.columns) >= 2:
                ts_data = ts_data.iloc[:, :2].copy()
                ts_data.columns = ['ds', 'y']
            else:
                raise ValueError(f"DataFrame –¥–ª—è –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ {modality} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞–Ω–Ω—ã—Ö")
        else:
            raise TypeError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {modality}: {type(ts_data)}")

        ts_data['ds'] = pd.to_datetime(ts_data['ds'])
        ts_data['y'] = pd.to_numeric(ts_data['y'], errors='coerce').clip(lower=0)
        ts_data = ts_data.dropna(subset=['y']).reset_index(drop=True)

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train –∏ test.
        train_df = ts_data[ts_data['ds'] < test_start].copy()
        test_df = ts_data[ts_data['ds'] >= test_start].copy()

        if len(test_df) == 0:
            print(f"‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∞ –≤ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏ {modality}")
            continue

        try:
            if modality in HIGH_FREQUENCY_MODALITIES:
                final_predictions, interval_forecast, prophet_model, auxiliary_model = run_hybrid_model_for_high_freq(train_df, test_df, modality)
                save_model(prophet_model, f"{modality}_prophet.pkl")
                save_model(auxiliary_model, f"{modality}_xgboost.pkl")
                model_type = 'prophet_xgboost'
            elif modality in SPECIALIZED_MODALITIES:
                final_predictions, interval_forecast, classifier_model, regressor_model, quantiles, max_vals = run_spike_detection_model_for_low_freq(train_df, test_df, modality)
                # –°–æ–∑–¥–∞–µ–º –µ–¥–∏–Ω—ã–π –æ–±—ä–µ–∫—Ç –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –í–°–ï–• –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –º–æ–¥–µ–ª–∏.
                spike_model_artifacts = {
                    'classifier': classifier_model,
                    'regressor_normal': regressor_model,
                    'quantiles': quantiles,
                    'max_vals': max_vals
                }
                save_model(spike_model_artifacts, f"{modality}_spike_model.pkl") # –°–æ—Ö—Ä–∞–Ω—è–µ–º –í–°–ï –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª
                model_type = 'spike_detection'
            else:
                raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—å: {modality}")

            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ —Ç–µ—Å—Ç–µ.
            test_y = test_df['y'].values
            mape = symmetric_mape(test_y, final_predictions)
            r2 = r2_score(test_y, final_predictions)
            rmse = np.sqrt(mean_squared_error(test_y, final_predictions))
            mae = mean_absolute_error(test_y, final_predictions)

            results[modality] = {
                'modality': modality,
                'model_type': model_type,
                'final_metrics': {'MAPE': mape, 'R2': r2, 'RMSE': rmse, 'MAE': mae},
                'test_actual': test_y,
                'test_final_pred': final_predictions,
            }

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–∞–±–ª–∏—Ü—É —Å –ø—Ä–æ–≥–Ω–æ–∑–∞–º–∏ –Ω–∞ —Ç–µ—Å—Ç–µ.
            forecast_table = test_df[['ds']].copy()
            forecast_table['y_true'] = test_y
            forecast_table['y_pred'] = final_predictions
            forecast_table['yhat_lower'] = interval_forecast.set_index('ds').loc[forecast_table['ds'], 'yhat_lower'].values
            forecast_table['yhat_upper'] = interval_forecast.set_index('ds').loc[forecast_table['ds'], 'yhat_upper'].values
            forecast_tables[modality] = forecast_table

            print("üìà –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–µ:")
            print(f"   MAPE: {mape:.3f}%")
            print(f"   R2: {r2:.4f}")
            print(f"   RMSE: {rmse:.1f}")
            print(f"   MAE: {mae:.1f}")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ {modality}: {str(e)}")
            import traceback
            traceback.print_exc()
            results[modality] = None
            forecast_tables[modality] = None

    # –í—ã–≤–æ–¥ —Å–≤–æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
    print("\n" + "="*80)
    print("üìã –°–í–û–î–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –û–ë–£–ß–ï–ù–ò–Ø")
    print("="*80)
    summary_data = []
    for modality, result in results.items():
        if result:
            summary_data.append({
                '–ú–æ–¥–∞–ª—å–Ω–æ—Å—Ç—å': modality,
                'MAPE': result['final_metrics']['MAPE'],
                'R2': result['final_metrics']['R2'],
                'RMSE': result['final_metrics']['RMSE'],
                'MAE': result['final_metrics']['MAE'],
                '–¢–∏–ø –º–æ–¥–µ–ª–∏': result['model_type']
            })

    if summary_data:
        summary_df = pd.DataFrame(summary_data)
        print(summary_df.to_string(index=False))
    else:
        print("–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è")
        summary_df = pd.DataFrame()

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –í–°–ï–• —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –æ–¥–∏–Ω Excel-—Ñ–∞–π–ª
    output_excel_path = os.path.join(RESULTS_DIR, "forecasting_results.xlsx")
    with pd.ExcelWriter(output_excel_path, engine='openpyxl') as writer:
        # 1. –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –º–µ—Ç—Ä–∏–∫.
        if not summary_df.empty:
            summary_df.to_excel(writer, sheet_name='–°–≤–æ–¥–Ω–∞—è_—Ç–∞–±–ª–∏—Ü–∞', index=False)
        # 2. –î–µ—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≥–Ω–æ–∑—ã –Ω–∞ —Ç–µ—Å—Ç–µ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏.
        for modality, forecast_df in forecast_tables.items():
            if forecast_df is not None:
                sheet_name = f"–¢–µ—Å—Ç_{modality}"[:31]  # Excel –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –¥–ª–∏–Ω—É –Ω–∞–∑–≤–∞–Ω–∏—è –ª–∏—Å—Ç–∞.
                forecast_df.to_excel(writer, sheet_name=sheet_name, index=False)

    print(f"\n‚úÖ‚úÖ‚úÖ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è –∏ –ø—Ä–æ–≥–Ω–æ–∑—ã –Ω–∞ —Ç–µ—Å—Ç–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {output_excel_path} ‚úÖ‚úÖ‚úÖ")

    return results, forecast_tables

if __name__ == "__main__":
    results, forecast_tables = train_and_evaluate_models()
    print(f"\nüéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫—É '{MODEL_DIR}'")